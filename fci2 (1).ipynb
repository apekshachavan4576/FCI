{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "turkish-venue",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Name: Apeksha Chavan\\nBE COMPS\\nUID:2017130013\\nFCI Exp 2: Experiment on studying different CNN architectures\\n\\nWhat is Convolutional Neural Network?\\nA Convolutional Neural Network (ConvNet/CNN) is a Deep Learning algorithm which can take in an input image, assign importance \\n(learnable weights and biases) to various aspects/objects in the image and be able to differentiate one from the other. \\nThe pre-processing required in a ConvNet is much lower as compared to other classification algorithms. \\nWhile in primitive methods filters are hand-engineered, with enough training, ConvNets have the ability to learn these filters/characteristics. \\nThe architecture of a ConvNet is analogous to that of the connectivity pattern of Neurons in the Human Brain and was inspired by the organization of the Visual Cortex. \\nIndividual neurons respond to stimuli only in a restricted region of the visual field known as the Receptive Field. \\nA collection of such fields overlap to cover the entire visual area.\\n\\nBasic Architecture\\nThere are two main parts to a CNN architecture\\nA convolution tool that separates and identifies the various features of the image for analysis in a process called as Feature Extraction\\nA fully connected layer that utilizes the output from the convolution process and predicts the class of the image based on the features extracted in previous stages.\\n\\nConvolution Layers \\nThere are three types of layers that make up the CNN which are the convolutional layers, pooling layers, \\nand fully-connected (FC) layers. When these layers are stacked, a CNN architecture will be formed.\\nIn addition to these three layers, there are two more important parameters which are the dropout layer and the activation function which are defined below.\\n\\n1. Convolutional Layer\\nThis layer is the first layer that is used to extract the various features from the input images. \\nIn this layer, the mathematical operation of convolution is performed between the input image and a filter of a particular size MxM. \\nBy sliding the filter over the input image, the dot product is taken between the filter and the parts of the input image with respect to the size of the filter (MxM).\\n\\nThe output is termed as the Feature map which gives us information about the image such as the corners and edges. \\nLater, this feature map is fed to other layers to learn several other features of the input image.\\n\\n2. Pooling Layer\\nIn most cases, a Convolutional Layer is followed by a Pooling Layer. \\nThe primary aim of this layer is to decrease the size of the convolved feature map to reduce the computational costs. \\nThis is performed by decreasing the connections between layers and independently operates on each feature map. \\nDepending upon method used, there are several types of Pooling operations.\\n\\nIn Max Pooling, the largest element is taken from feature map. \\nAverage Pooling calculates the average of the elements in a predefined sized Image section. \\nThe total sum of the elements in the predefined section is computed in Sum Pooling. \\nThe Pooling Layer usually serves as a bridge between the Convolutional Layer and the FC Layer\\n\\n3. Fully Connected Layer\\nThe Fully Connected (FC) layer consists of the weights and biases along with the neurons and is used to connect the neurons between two different layers. \\nThese layers are usually placed before the output layer and form the last few layers of a CNN Architecture.\\n\\nIn this, the input image from the previous layers are flattened and fed to the FC layer. \\nThe flattened vector then undergoes few more FC layers where the mathematical functions operations usually take place. \\nIn this stage, the classification process begins to take place.\\n\\n4. Dropout\\nUsually, when all the features are connected to the FC layer, it can cause overfitting in the training dataset. \\nOverfitting occurs when a particular model works so well on the training data causing a negative impact in the model’s performance when used on a new data.\\n\\nTo overcome this problem, a dropout layer is utilised wherein a few neurons are dropped from the neural network during training process resulting in reduced size of the model. \\nOn passing a dropout of 0.3, 30% of the nodes are dropped out randomly from the neural network.\\n\\n5. Activation Functions\\nFinally, one of the most important parameters of the CNN model is the activation function. \\nThey are used to learn and approximate any kind of continuous and complex relationship between variables of the network. \\nIn simple words, it decides which information of the model should fire in the forward direction and which ones should not at the end of the network.\\n\\nIt adds non-linearity to the network. There are several commonly used activation functions such as the ReLU, Softmax, tanH and the Sigmoid functions. \\nEach of these functions have a specific usage. For a binary classification CNN model, sigmoid and softmax functions are preferred an for a multi-class classification, generally softmax us used.'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Name: Apeksha Chavan\n",
    "BE COMPS\n",
    "UID:2017130013\n",
    "FCI Exp 2: Experiment on studying different CNN architectures\n",
    "\n",
    "What is Convolutional Neural Network?\n",
    "A Convolutional Neural Network (ConvNet/CNN) is a Deep Learning algorithm which can take in an input image, assign importance \n",
    "(learnable weights and biases) to various aspects/objects in the image and be able to differentiate one from the other. \n",
    "The pre-processing required in a ConvNet is much lower as compared to other classification algorithms. \n",
    "While in primitive methods filters are hand-engineered, with enough training, ConvNets have the ability to learn these filters/characteristics. \n",
    "The architecture of a ConvNet is analogous to that of the connectivity pattern of Neurons in the Human Brain and was inspired by the organization of the Visual Cortex. \n",
    "Individual neurons respond to stimuli only in a restricted region of the visual field known as the Receptive Field. \n",
    "A collection of such fields overlap to cover the entire visual area.\n",
    "\n",
    "Basic Architecture\n",
    "There are two main parts to a CNN architecture\n",
    "A convolution tool that separates and identifies the various features of the image for analysis in a process called as Feature Extraction\n",
    "A fully connected layer that utilizes the output from the convolution process and predicts the class of the image based on the features extracted in previous stages.\n",
    "\n",
    "Convolution Layers \n",
    "There are three types of layers that make up the CNN which are the convolutional layers, pooling layers, \n",
    "and fully-connected (FC) layers. When these layers are stacked, a CNN architecture will be formed.\n",
    "In addition to these three layers, there are two more important parameters which are the dropout layer and the activation function which are defined below.\n",
    "\n",
    "1. Convolutional Layer\n",
    "This layer is the first layer that is used to extract the various features from the input images. \n",
    "In this layer, the mathematical operation of convolution is performed between the input image and a filter of a particular size MxM. \n",
    "By sliding the filter over the input image, the dot product is taken between the filter and the parts of the input image with respect to the size of the filter (MxM).\n",
    "\n",
    "The output is termed as the Feature map which gives us information about the image such as the corners and edges. \n",
    "Later, this feature map is fed to other layers to learn several other features of the input image.\n",
    "\n",
    "2. Pooling Layer\n",
    "In most cases, a Convolutional Layer is followed by a Pooling Layer. \n",
    "The primary aim of this layer is to decrease the size of the convolved feature map to reduce the computational costs. \n",
    "This is performed by decreasing the connections between layers and independently operates on each feature map. \n",
    "Depending upon method used, there are several types of Pooling operations.\n",
    "\n",
    "In Max Pooling, the largest element is taken from feature map. \n",
    "Average Pooling calculates the average of the elements in a predefined sized Image section. \n",
    "The total sum of the elements in the predefined section is computed in Sum Pooling. \n",
    "The Pooling Layer usually serves as a bridge between the Convolutional Layer and the FC Layer\n",
    "\n",
    "3. Fully Connected Layer\n",
    "The Fully Connected (FC) layer consists of the weights and biases along with the neurons and is used to connect the neurons between two different layers. \n",
    "These layers are usually placed before the output layer and form the last few layers of a CNN Architecture.\n",
    "\n",
    "In this, the input image from the previous layers are flattened and fed to the FC layer. \n",
    "The flattened vector then undergoes few more FC layers where the mathematical functions operations usually take place. \n",
    "In this stage, the classification process begins to take place.\n",
    "\n",
    "4. Dropout\n",
    "Usually, when all the features are connected to the FC layer, it can cause overfitting in the training dataset. \n",
    "Overfitting occurs when a particular model works so well on the training data causing a negative impact in the model’s performance when used on a new data.\n",
    "\n",
    "To overcome this problem, a dropout layer is utilised wherein a few neurons are dropped from the neural network during training process resulting in reduced size of the model. \n",
    "On passing a dropout of 0.3, 30% of the nodes are dropped out randomly from the neural network.\n",
    "\n",
    "5. Activation Functions\n",
    "Finally, one of the most important parameters of the CNN model is the activation function. \n",
    "They are used to learn and approximate any kind of continuous and complex relationship between variables of the network. \n",
    "In simple words, it decides which information of the model should fire in the forward direction and which ones should not at the end of the network.\n",
    "\n",
    "It adds non-linearity to the network. There are several commonly used activation functions such as the ReLU, Softmax, tanH and the Sigmoid functions. \n",
    "Each of these functions have a specific usage. For a binary classification CNN model, sigmoid and softmax functions are preferred an for a multi-class classification, generally softmax us used.'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "retained-moore",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "metallic-trauma",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "(train_x, train_y), (test_x, test_y) = keras.datasets.mnist.load_data()\n",
    "train_x = train_x / 255.0\n",
    "test_x = test_x / 255.0\n",
    "\n",
    "train_x = tf.expand_dims(train_x, 3)\n",
    "test_x = tf.expand_dims(test_x, 3)\n",
    "\n",
    "val_x = train_x[:5000]\n",
    "val_y = train_y[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "parliamentary-catalyst",
   "metadata": {},
   "outputs": [],
   "source": [
    "lenet_5_model = keras.models.Sequential([\n",
    "    keras.layers.Conv2D(6, kernel_size=5, strides=1,  activation='tanh', input_shape=train_x[0].shape, padding='same'), #C1 #tanh = Hyperbolic tangent\n",
    "    keras.layers.AveragePooling2D(), #S2    #activation function = Sigmoid\n",
    "    keras.layers.Conv2D(16, kernel_size=5, strides=1, activation='tanh', padding='valid'), #C3\n",
    "    keras.layers.AveragePooling2D(), #S4    #activation function = Sigmoid\n",
    "    keras.layers.Flatten(), #Flatten\n",
    "    keras.layers.Dense(120, activation='tanh'), #C5\n",
    "    keras.layers.Dense(84, activation='tanh'), #F6\n",
    "    keras.layers.Dense(10, activation='softmax') #Output layer  (In total 7 layers(1 Flatten layer) + 1 Output layer)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "southeast-progressive",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we can compile and build the model\n",
    "lenet_5_model.compile(optimizer='adam', loss=keras.losses.sparse_categorical_crossentropy, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "metropolitan-woman",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "root_logdir = os.path.join(os.curdir, \"logs\\\\fit\\\\\")\n",
    "\n",
    "def get_run_logdir():\n",
    "    run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\n",
    "    return os.path.join(root_logdir, run_id)\n",
    "\n",
    "run_logdir = get_run_logdir()\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "blond-fiber",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 32s 17ms/step - loss: 0.2289 - accuracy: 0.9314 - val_loss: 0.0874 - val_accuracy: 0.9730\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 30s 16ms/step - loss: 0.0857 - accuracy: 0.9736 - val_loss: 0.0596 - val_accuracy: 0.9816\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 26s 14ms/step - loss: 0.0570 - accuracy: 0.9830 - val_loss: 0.0355 - val_accuracy: 0.9896\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 26s 14ms/step - loss: 0.0447 - accuracy: 0.9860 - val_loss: 0.0329 - val_accuracy: 0.9902\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 28s 15ms/step - loss: 0.0355 - accuracy: 0.9888 - val_loss: 0.0259 - val_accuracy: 0.9922\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x22c216ff070>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lenet_5_model.fit(train_x, train_y, epochs=5, validation_data=(val_x, val_y), callbacks=[tensorboard_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "corrected-letters",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 2s 6ms/step - loss: 0.0493 - accuracy: 0.9844\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0492842011153698, 0.9843999743461609]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lenet_5_model.evaluate(test_x, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "expected-direction",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CONCLUSION:\\nIn this experiment, I have used the LENET-5 architecture, trained and tested the MNIST dataset,\\nKeras presents a Sequential API for stacking layers of the neural network on top of each other, so for the CNN architecture layers\\nI have used  the Keras tools required to implement the classification model.\\nAfter training, the model achieves a validation accuracy of over 90%.\\nAfter training my model, I was able to achieve 98% accuracy on the test dataset, which is quite useful for such a simple network.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''CONCLUSION:\n",
    "In this experiment, I have used the LENET-5 architecture, trained and tested the MNIST dataset,\n",
    "Keras presents a Sequential API for stacking layers of the neural network on top of each other, so for the CNN architecture layers\n",
    "I have used  the Keras tools required to implement the classification model.\n",
    "After training, the model achieves a validation accuracy of over 90%.\n",
    "After training my model, I was able to achieve 98% accuracy on the test dataset, which is quite useful for such a simple network.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "designed-adelaide",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
